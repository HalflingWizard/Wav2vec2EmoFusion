{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Iqi_jAjJTQD"
      },
      "source": [
        "# 🚀 Install, Import, and Log In"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2la1s_6HJjf"
      },
      "source": [
        "## Install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6v_hr9jhHMU1",
        "outputId": "f623204e-19cd-47c5-ca4d-41e86a022e3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.0/499.0 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 KB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m555.3/555.3 KB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m59.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 KB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.2/517.2 KB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.11.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m130.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 KB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.3/109.3 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for accelerate (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q speechbrain --upgrade -qqq\n",
        "!pip install wandb --upgrade -qqq\n",
        "!pip install torchmetrics --upgrade -qqq\n",
        "!pip install torchinfo --upgrade -qqq\n",
        "!pip install onnx --upgrade -qqq\n",
        "!pip install transformers -qqq\n",
        "!pip install einops --upgrade -qqq\n",
        "!pip install nlpaug --upgrade -qqq\n",
        "!pip install pytorch-metric-learning --upgrade -qqq\n",
        "!pip install git+https://github.com/huggingface/accelerate -qqq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCiM2wnCJmOK"
      },
      "source": [
        "## Weights and Biases login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zC35lijVHUqt",
        "outputId": "ded40cd7-1d68-47f9-d937-171d37bee1b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import wandb\n",
        "wandb.login() # Log in with your wandb creditentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PoabZ7sStuW8"
      },
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PuQI4M91tf1J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from torchmetrics import Accuracy\n",
        "from torchinfo import summary\n",
        "\n",
        "import torchaudio\n",
        "import nlpaug.augmenter.audio as naa\n",
        "import nlpaug.flow as naf\n",
        "\n",
        "from speechbrain.nnet.losses import AdditiveAngularMargin, LogSoftmaxWrapper\n",
        "from speechbrain.lobes.models.ECAPA_TDNN import AttentiveStatisticsPooling\n",
        "\n",
        "from pytorch_metric_learning import losses, samplers\n",
        "\n",
        "import onnx\n",
        "from onnx import shape_inference\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import os\n",
        "import math\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import pickle\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "from transformers import get_cosine_with_hard_restarts_schedule_with_warmup\n",
        "\n",
        "from accelerate import Accelerator\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkDPDFgUOckn"
      },
      "source": [
        "## Set-up better GPU stack trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2QPb2uyOdUr"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-blQEzkoFFY1"
      },
      "source": [
        "## Ensure deterministic behavior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TVL0mIS9FDRx"
      },
      "outputs": [],
      "source": [
        "os.environ['PYTHONHASHSEED'] = '42'\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvzQ5kNOzzYF"
      },
      "source": [
        "## Set up GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjviwq78z1Jt"
      },
      "outputs": [],
      "source": [
        "accelerator = Accelerator(mixed_precision = 'fp16')\n",
        "device = accelerator.device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMazsAf6J81k"
      },
      "source": [
        "# 👩‍🔬 Define the Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UexlatABOlRp"
      },
      "source": [
        "## Sweep Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVkDld6pOimg"
      },
      "outputs": [],
      "source": [
        "sweep_config = {\n",
        "    'method': 'grid'\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Pw-UBRtoEAy"
      },
      "source": [
        "## Sweep Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fN0oLDAvPBQM"
      },
      "outputs": [],
      "source": [
        "parameters_dict = {\n",
        "    'fold': {\n",
        "        # 'values': [('01','02'), ('03','04'), ('05','06'), ('07','08'), ('09','10'), ('11','12'), ('13','14'), ('15','16'), ('17','18'), ('19','20'), ('21','22'), ('23','24')] # RAVDESS\n",
        "        'values': [[\"03\"], [\"08\"], [\"09\"], [\"10\"], [\"11\"], [\"12\"], [\"13\"], [\"14\"], [\"15\"], [\"16\"]]\n",
        "    }\n",
        "  }\n",
        "\n",
        "sweep_config['parameters'] = parameters_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVBzY0pLpHHQ"
      },
      "source": [
        "## Training General Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xro74JIGpMqC"
      },
      "outputs": [],
      "source": [
        "parameters_dict.update({\n",
        "    'epochs':{\n",
        "      'value': 40\n",
        "    },\n",
        "    'batch_size':{\n",
        "        'value': 32\n",
        "    },\n",
        "    'learning_rate':{\n",
        "        'value': 5e-3\n",
        "    },\n",
        "    'weight_decay':{\n",
        "        'value': 9e-3\n",
        "    },\n",
        "    'warmup_steps':{\n",
        "        'value': 10\n",
        "    },\n",
        "    'num_cycles':{\n",
        "        'value': 1\n",
        "    },\n",
        "    'loss':{\n",
        "        'value': 'AF'\n",
        "    },\n",
        "    'optimizer':{\n",
        "        'value': 'AdamW'\n",
        "    },\n",
        "    'alpha':{\n",
        "        'value': 1\n",
        "    },\n",
        "    'beta':{\n",
        "        'value': 1\n",
        "    },\n",
        "    'temporal_average':{\n",
        "        'value': 'asp'\n",
        "    },\n",
        "    'scale':{\n",
        "        'value': 1\n",
        "    },\n",
        "    'margin':{\n",
        "        'value': 0.2\n",
        "    },\n",
        "    'layer_selection':{\n",
        "        'value': 'asp'\n",
        "    },\n",
        "    'layer_attn_dim':{\n",
        "        'value': 32\n",
        "    },\n",
        "    'time_attn_dim':{\n",
        "        'value': 256\n",
        "    },\n",
        "    'emo_emdb_ratio':{\n",
        "        'value': 0.5\n",
        "    },\n",
        "    'hidden_size':{\n",
        "        'value': 64\n",
        "    }\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTyc78OIuMV8"
      },
      "source": [
        "## Speech Features Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SOM1ZNjOuPsm"
      },
      "outputs": [],
      "source": [
        "parameters_dict.update({\n",
        "    'max_length':{\n",
        "        'value': 350\n",
        "    },\n",
        "    'sample_rate':{\n",
        "        'value': 16000\n",
        "    },\n",
        "    'aug_prob':{\n",
        "        'value': 0.0\n",
        "    },\n",
        "    'proj_drop':{\n",
        "        'value': 0.2\n",
        "    },\n",
        "    'attn_drop':{\n",
        "        'value': 0.0\n",
        "    }\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBKL4uMZpWiB"
      },
      "source": [
        "## Model Specific Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viMclRJb98Kf"
      },
      "outputs": [],
      "source": [
        "parameters_dict.update({\n",
        "    'input_size':{\n",
        "        'value': (12, 350, 768)\n",
        "    },\n",
        "    'embeddings':{\n",
        "        'value': 768\n",
        "    },\n",
        "    'num_heads':{\n",
        "        'value': 2\n",
        "    }\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eynOxEhpoa2B"
      },
      "source": [
        "## Dataset and input Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ard4-CxgoeuB"
      },
      "outputs": [],
      "source": [
        "parameters_dict.update({\n",
        "    'dataset':{\n",
        "        'value':\"EMODB\"\n",
        "    },\n",
        "    'path':{\n",
        "        'value':'kntu-asp-dl/EMODB/EMODB:wav2vec_fp16_noft_base'\n",
        "    },\n",
        "    'layers':{\n",
        "        'value': list(range(1,13))\n",
        "    },\n",
        "    'classes':{\n",
        "        'value': 7\n",
        "    },\n",
        "    'class_names':{\n",
        "        'value':['anger', 'anxiety/fear', 'boredom', 'disgust', 'happiness', 'neutral', 'sadness']\n",
        "    },\n",
        "    'input_channels':{\n",
        "        'value': 1\n",
        "    }\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cF7z2gP4oHaS"
      },
      "source": [
        "## Metadata about the run"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FD0J6xZRoTn6"
      },
      "outputs": [],
      "source": [
        "parameters_dict.update({\n",
        "    'project_name':{\n",
        "        'value':\"Thesis\"\n",
        "    }\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktd3_4lPaoiZ"
      },
      "source": [
        "# 🧹 Initialize the Sweep"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-slr8i-S5aJ",
        "outputId": "554c3417-efbb-4d9e-ac9b-5798c61d6637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is sweep_id? (leave out if this is first sweep) \n",
            "Create sweep with ID: e7czjb2r\n",
            "Sweep URL: https://wandb.ai/halflingwizard/Thesis/sweeps/e7czjb2r\n"
          ]
        }
      ],
      "source": [
        "save_model = False\n",
        "sweep_id = input('What is sweep_id? (leave out if this is first sweep) ')\n",
        "if sweep_id==\"\":\n",
        "    sweep_id = wandb.sweep(sweep_config, project=parameters_dict['project_name']['value'])\n",
        "    save_model = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Zqvq1fCnyPD"
      },
      "source": [
        "# 🚰 Constructing the Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTnN4fPzaoiZ"
      },
      "source": [
        "## Define pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tyG1K8cDaoia"
      },
      "outputs": [],
      "source": [
        "def model_pipeline(hyperparameters=None):\n",
        "\n",
        "    # tell wandb to get started\n",
        "    with wandb.init(config=hyperparameters):\n",
        "\n",
        "      # access all HPs through wandb.config, so logging matches execution!\n",
        "      config = wandb.config\n",
        "\n",
        "      # download dataset\n",
        "      paths = []\n",
        "      for i in config.layers:\n",
        "        dataset = wandb.use_artifact(config.path+f'_t{i}', type='dataset')\n",
        "        # dataset path\n",
        "        path = dataset.download()\n",
        "        paths.append(path)\n",
        "\n",
        "      # make the model, data, and optimization problem\n",
        "      model, best_model, train_loader, test_loader, criterion, optimizer, scheduler = make(config, paths)\n",
        "      try:\n",
        "        print(summary(model, (config.batch_size, config.input_size[0], config.input_size[1], config.input_size[2])))\n",
        "      except:\n",
        "        print('⚠️ Something is wrong with TorchInfo.')\n",
        "\n",
        "      # and set-up the metric\n",
        "      metric = Metric(config.classes, config.layers, train_loader.dataset.prior_prob)\n",
        "\n",
        "      # and use them to train the model\n",
        "      train(model, train_loader, test_loader, criterion, optimizer, scheduler, metric, config)\n",
        "\n",
        "      # load the best model from training\n",
        "      best_model.load_state_dict(torch.load('model.pkl'))\n",
        "\n",
        "      # and test its final performance\n",
        "      test(best_model, test_loader, criterion, metric, config)\n",
        "\n",
        "      pickle_artifact = wandb.Artifact(\n",
        "          name = f'Model.pkl',\n",
        "          type=\"model\",\n",
        "          metadata=dict(config))\n",
        "\n",
        "      wandb.save(\"model.pkl\")\n",
        "      pickle_artifact.add_file(\"model.pkl\")\n",
        "      wandb.log_artifact(pickle_artifact)\n",
        "\n",
        "\n",
        "      if save_model == True:\n",
        "        try:\n",
        "          # Save the model in the exchangeable ONNX format\n",
        "          onnx_artifact = wandb.Artifact(\n",
        "              name = f'Model.onnx',\n",
        "              type=\"model\",\n",
        "              metadata=dict(config))\n",
        "\n",
        "          # Params for ONNX\n",
        "          dummy_input = torch.randn(config.batch_size, config.input_size, device = device).to(torch.float16)\n",
        "          input_names = [ \"input\" ]\n",
        "          output_names = [ \"output\" ]\n",
        "\n",
        "          # export model as ONNX file\n",
        "          torch.onnx.export(best_model,\n",
        "                            dummy_input,\n",
        "                            \"pytorch_model.onnx\",\n",
        "                            verbose=False,\n",
        "                            input_names=input_names,\n",
        "                            output_names=output_names,\n",
        "                            export_params=True,\n",
        "                            dynamic_axes={'input' : {0 : 'batch_size'},\n",
        "                                          'output' : {0 : 'batch_size'}})\n",
        "          onnx.save(onnx.shape_inference.infer_shapes(onnx.load(\"pytorch_model.onnx\")), \"model.onnx\")\n",
        "\n",
        "          wandb.save(\"model.onnx\")\n",
        "\n",
        "          onnx_artifact.add_file(\"model.onnx\")\n",
        "\n",
        "          wandb.log_artifact(onnx_artifact)\n",
        "        except:\n",
        "          print(\"⚠️ Couldn't save the model.\")\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvCxNvYU2qJ4"
      },
      "source": [
        "## Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NISGwudr2pgZ"
      },
      "outputs": [],
      "source": [
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha = 0.8, gamma = 2):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "\n",
        "        #comment out if your model contains a sigmoid or equivalent activation layer\n",
        "        sig = nn.Sigmoid()\n",
        "        inputs = sig(inputs)\n",
        "\n",
        "        #flatten label and prediction tensors\n",
        "        inputs = inputs.view(-1)\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        #first compute binary cross-entropy\n",
        "        BCE = F.binary_cross_entropy(inputs, targets, reduction='mean')\n",
        "        BCE_EXP = torch.exp(-BCE)\n",
        "        focal_loss = self.alpha * (1-BCE_EXP)**self.gamma * BCE\n",
        "\n",
        "        return focal_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlYzOu0gRD7y"
      },
      "outputs": [],
      "source": [
        "class DaviesBouldin(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DaviesBouldin, self).__init__()\n",
        "\n",
        "    def pytorch_euclidean(self, a, b):\n",
        "        return torch.sqrt(torch.sum((a-b)**2))\n",
        "\n",
        "    def forward(self, X, labels):\n",
        "\n",
        "        n_cluster = len(torch.bincount(labels.int()))\n",
        "        cluster_k = [X[labels == k] for k in range(n_cluster)]\n",
        "        centroids = [torch.mean(k, dim = 0) for k in cluster_k]\n",
        "        variances = [torch.mean(torch.Tensor([self.pytorch_euclidean(p, centroids[i]) for p in k])) for i, k in enumerate(cluster_k)]\n",
        "        db = []\n",
        "\n",
        "        for i in range(n_cluster):\n",
        "            for j in range(n_cluster):\n",
        "                if j != i:\n",
        "                    db.append((variances[i] + variances[j]) / self.pytorch_euclidean(centroids[i], centroids[j]))\n",
        "        if n_cluster == 1:\n",
        "          db.append(0)\n",
        "\n",
        "        return(max(db) / n_cluster)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_I49aUHAKKiP"
      },
      "outputs": [],
      "source": [
        "class CenterLoss(nn.Module):\n",
        "    \"\"\"Center loss.\n",
        "\n",
        "    Reference:\n",
        "    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.\n",
        "\n",
        "    Args:\n",
        "        num_classes (int): number of classes.\n",
        "        feat_dim (int): feature dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, feat_dim=2, use_gpu=True):\n",
        "        super(CenterLoss, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.feat_dim = feat_dim\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        if self.use_gpu:\n",
        "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n",
        "        else:\n",
        "            self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim))\n",
        "\n",
        "    def forward(self, x, labels):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: feature matrix with shape (batch_size, feat_dim).\n",
        "            labels: ground truth labels with shape (batch_size).\n",
        "        \"\"\"\n",
        "        batch_size = x.size(0)\n",
        "        distmat = torch.pow(x, 2).sum(dim=1, keepdim=True).expand(batch_size, self.num_classes) + \\\n",
        "                  torch.pow(self.centers, 2).sum(dim=1, keepdim=True).expand(self.num_classes, batch_size).t()\n",
        "        distmat.addmm_(x, self.centers.t(), beta=1, alpha=-2)\n",
        "\n",
        "        classes = torch.arange(self.num_classes).long()\n",
        "        if self.use_gpu: classes = classes.cuda()\n",
        "        labels = labels.unsqueeze(1).expand(batch_size, self.num_classes)\n",
        "        mask = labels.eq(classes.expand(batch_size, self.num_classes))\n",
        "\n",
        "        dist = distmat * mask.float()\n",
        "        loss = dist.clamp(min=1e-12, max=1e+12).sum() / batch_size\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjWJT91n7cUs"
      },
      "outputs": [],
      "source": [
        "class RingLoss(nn.Module):\n",
        "    \"\"\"\n",
        "    Refer to paper\n",
        "    Ring loss: Convex Feature Normalization for Face Recognition\n",
        "    \"\"\"\n",
        "    def __init__(self, type='L2', loss_weight=1.0):\n",
        "        super(RingLoss, self).__init__()\n",
        "        self.radius = nn.Parameter(torch.Tensor(1)).cuda()\n",
        "        self.radius.data.fill_(-1)\n",
        "        self.loss_weight = loss_weight\n",
        "        self.type = type\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.pow(2).sum(dim=1).pow(0.5)\n",
        "        if self.radius.data[0] < 0: # Initialize the radius with the mean feature norm of first iteration\n",
        "            self.radius.data.fill_(x.mean().item())\n",
        "        if self.type == 'L1': # Smooth L1 Loss\n",
        "            loss1 = F.smooth_l1_loss(x, self.radius.expand_as(x)).mul_(self.loss_weight)\n",
        "            loss2 = F.smooth_l1_loss(self.radius.expand_as(x), x).mul_(self.loss_weight)\n",
        "            ringloss = loss1 + loss2\n",
        "        elif self.type == 'auto': # Divide the L2 Loss by the feature's own norm\n",
        "            diff = x.sub(self.radius.expand_as(x)) / (x.mean().detach().clamp(min=0.5))\n",
        "            diff_sq = torch.pow(torch.abs(diff), 2).mean()\n",
        "            ringloss = diff_sq.mul_(self.loss_weight)\n",
        "        else: # L2 Loss, if not specified\n",
        "            diff = x.sub(self.radius.expand_as(x))\n",
        "            diff_sq = torch.pow(torch.abs(diff), 2).mean()\n",
        "            ringloss = diff_sq.mul_(self.loss_weight)\n",
        "        return ringloss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udNcFzWY4EGP"
      },
      "outputs": [],
      "source": [
        "class CrossEntropy(nn.Module):\n",
        "    def __init__(self, num_classes = 4, feat_dim = 768):\n",
        "        super(CrossEntropy, self).__init__()\n",
        "        self.CELoss = nn.CrossEntropyLoss()\n",
        "        self.RingLoss = RingLoss()\n",
        "\n",
        "    def forward(self, inputs, embeddings, targets):\n",
        "\n",
        "        return self.CELoss(inputs, targets) + self.RingLoss(embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ssy2xTCnvTse"
      },
      "outputs": [],
      "source": [
        "class AFLoss(nn.Module):\n",
        "    def __init__(self, margin, scale, num_layers):\n",
        "        super().__init__()\n",
        "        self.AAMLoss = nn.ModuleList([\n",
        "                                      LogSoftmaxWrapper(AdditiveAngularMargin(margin = margin, scale=scale))\n",
        "                                      for _ in range(num_layers)\n",
        "                                      ]\n",
        "                                    )\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        targets, ses_sex_emo = targets\n",
        "        labels = process_labels(targets, self.AAMLoss[0])\n",
        "        loss = 0\n",
        "        for i, layer in enumerate(self.AAMLoss):\n",
        "          loss += layer(inputs[:, i], labels)\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Metrics"
      ],
      "metadata": {
        "id": "D3tz8q8dsb-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Metric:\n",
        "  def __init__(self, num_classes, layers, prior_probabilities):\n",
        "\n",
        "    self.UA = Accuracy(task = 'multiclass',\n",
        "                       num_classes = num_classes,\n",
        "                       average = 'macro')\n",
        "    self.prior_prob = prior_probabilities\n",
        "    self.layers = layers\n",
        "\n",
        "  def majority_vote(self, outputs):\n",
        "    sm = nn.Softmax(dim=2)\n",
        "    outputs = sm(outputs)\n",
        "\n",
        "    predicted, _ = torch.mode(outputs.argmax(dim=2), dim=1)\n",
        "    return predicted.squeeze().tolist()\n",
        "\n",
        "  def fixed_weighted_mean(self, outputs):\n",
        "    weights = torch.tensor([0.070, 0.072, 0.076, 0.078,\n",
        "                            0.081, 0.084, 0.083, 0.085,\n",
        "                            0.082, 0.075, 0.071, 0.078],\n",
        "                           device = device) # from pepnio et al.\n",
        "    weights = weights[[id-1 for id in self.layers]]\n",
        "    sm = torch.nn.Softmax(dim=1)\n",
        "\n",
        "    w = sm(weights.unsqueeze(0).unsqueeze(2))\n",
        "    predicted = torch.argmax(torch.sum(outputs*w, dim=1, keepdim=True), dim=2)\n",
        "\n",
        "    return predicted.squeeze().tolist()\n",
        "\n",
        "  def max_max(self, outputs):\n",
        "    sm = nn.Softmax(dim=2)\n",
        "    outputs = sm(outputs)\n",
        "\n",
        "    outputs, _ = torch.max(outputs, dim=1)\n",
        "\n",
        "    predicted = torch.argmax(outputs, dim=1)\n",
        "    return predicted.squeeze().tolist()\n",
        "\n",
        "  def distribution_summation(self, outputs):\n",
        "    sm = nn.Softmax(dim=2)\n",
        "    outputs = sm(outputs)\n",
        "\n",
        "    predicted = torch.argmax(outputs.sum(dim=1), dim=1)\n",
        "    return predicted.squeeze().tolist()\n",
        "\n",
        "  def dempster_shafer(self, outputs):\n",
        "    sm = nn.Softmax(dim=2)\n",
        "    outputs = sm(outputs)\n",
        "\n",
        "    bpa = 1 - torch.prod((1-outputs), dim=1)\n",
        "    A = torch.sum(bpa/(1-bpa)+1, dim=1, keepdim=True)\n",
        "    Bel = (1/A)*(bpa/(1-bpa))\n",
        "    predicted = torch.argmax(Bel, dim=1)\n",
        "\n",
        "    return predicted.squeeze().tolist()\n",
        "\n",
        "  def naive_bayes(self, outputs):\n",
        "    sm = nn.Softmax(dim=2)\n",
        "    outputs = sm(outputs)\n",
        "    prior = torch.tensor(self.prior_prob, device = device).unsqueeze(0).unsqueeze(1)\n",
        "\n",
        "    predicted = torch.argmax(prior*torch.prod(outputs/prior, dim=1, keepdim=True), dim=2)\n",
        "\n",
        "    return predicted.squeeze().tolist()\n",
        "\n",
        "  def entropy_weighting(self, outputs):\n",
        "    sm = nn.Softmax(dim=2)\n",
        "    outputs = sm(outputs)\n",
        "\n",
        "    e = -torch.sum(outputs*torch.log(outputs), dim=2, keepdim=True)\n",
        "    predicted = torch.argmax(torch.sum((outputs==torch.max(outputs, dim=2,keepdim=True)[0]).int()*e, dim=1, keepdim=True), dim=2)\n",
        "\n",
        "    return predicted.squeeze().tolist()\n",
        "\n",
        "  def density_based_weighting(self, outputs):\n",
        "    sm = nn.Softmax(dim=2)\n",
        "    outputs = sm(outputs)\n",
        "\n",
        "    predicted = torch.argmax(torch.sum((outputs==torch.max(outputs, dim=2,keepdim=True)[0]).int()*outputs, dim=1, keepdim=True), dim=2)\n",
        "\n",
        "    return predicted.squeeze().tolist()\n",
        "\n",
        "  def entropy_weighted_mean(self, outputs):\n",
        "    sm = nn.Softmax(dim=2)\n",
        "    outputs = sm(outputs) #p\n",
        "\n",
        "    entropy = torch.sum(outputs * torch.log(1/outputs), dim=2, keepdim=True)\n",
        "    weight = 1/entropy\n",
        "    outputs = outputs * weight\n",
        "\n",
        "    predicted = torch.argmax(outputs.sum(dim=1), dim=1)\n",
        "    return predicted.squeeze().tolist()\n",
        "\n",
        "  def calculate_acc(self, predictions, labels):\n",
        "    y_true = labels[0].tolist()\n",
        "    y_pred = predictions\n",
        "\n",
        "    return self.UA(torch.tensor(y_pred), torch.tensor(y_true))"
      ],
      "metadata": {
        "id": "6Qhy7mFG25F2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUURyymWaoia"
      },
      "source": [
        "## Define `make`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "20EC9FZGaoia"
      },
      "outputs": [],
      "source": [
        "def make(config,  paths):\n",
        "    # Make the data\n",
        "    train, test =  get_data(config,  paths, train=True), get_data(config,  paths, train=False)\n",
        "    train_loader = make_loader(train, batch_size=config.batch_size)\n",
        "    test_loader = make_loader(test, batch_size=config.batch_size)\n",
        "\n",
        "    # Make the model\n",
        "    model = Model(config)\n",
        "\n",
        "    # Make best model\n",
        "    best_model = Model(config)\n",
        "\n",
        "    # Make the loss and optimizer\n",
        "    criterion = get_loss(config.loss, config)\n",
        "    params = list(model.parameters()) + list(criterion.parameters())\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params,\n",
        "                                  lr = config.learning_rate,\n",
        "                                  weight_decay = config.weight_decay)\n",
        "\n",
        "    # Make the scheduler\n",
        "    scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer,\n",
        "                                                                   num_warmup_steps = config.warmup_steps,\n",
        "                                                                   num_training_steps = config.epochs,\n",
        "                                                                   num_cycles = config.num_cycles)\n",
        "\n",
        "    model, best_model, optimizer, train_loader, test_loader = accelerator.prepare(model, best_model, optimizer, train_loader, test_loader)\n",
        "\n",
        "    return model, best_model, train_loader, test_loader, criterion, optimizer, scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt-w26881oux"
      },
      "outputs": [],
      "source": [
        "def get_loss(loss_function, config):\n",
        "  if loss_function.startswith('CE'):\n",
        "    loss = CrossEntropy(config.classes, config.embeddings)\n",
        "  elif loss_function.startswith(\"MM\"):\n",
        "    loss = nn.MultiMarginLoss()\n",
        "  elif loss_function.startswith(\"MLSM\"):\n",
        "    loss = nn.MultiLabelSoftMarginLoss()\n",
        "  elif loss_function.startswith(\"FL\"):\n",
        "    loss = FocalLoss(alpha=0.75, gamma = 2)\n",
        "  elif loss_function.startswith(\"BCE\"):\n",
        "    loss = nn.BCEWithLogitsLoss()\n",
        "  elif loss_function.startswith(\"AF\"):\n",
        "    loss = AFLoss(margin = config.margin, scale=config.scale, num_layers=len(config.layers))\n",
        "  return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ5bbKTBMmFF"
      },
      "source": [
        "# 🔊 Speech Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkMn9HuKnSub"
      },
      "source": [
        "## Reading wav file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4HdCJ_I7MqUb"
      },
      "outputs": [],
      "source": [
        "def get_waveform(file_path, sample_rate):\n",
        "  wf, sr = torchaudio.load(file_path)\n",
        "  resample = torchaudio.transforms.Resample(sr, sample_rate)\n",
        "  waveform = resample(wf)\n",
        "  return waveform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xClzeKHPos32"
      },
      "source": [
        "## Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNCZAg9totoA"
      },
      "outputs": [],
      "source": [
        "def augment(waveform, aug_prob, sample_rate):\n",
        "\n",
        "  aug = naf.Sometimes([\n",
        "    naa.SpeedAug(factor=(0.8, 1.2)),\n",
        "    naa.LoudnessAug(),\n",
        "    naa.VtlpAug(sample_rate)\n",
        "    ],\n",
        "    aug_p = aug_prob)\n",
        "\n",
        "  waveform = waveform.numpy()\n",
        "  waveform = waveform.reshape(-1)\n",
        "  waveform = aug.augment(waveform)\n",
        "  waveform = torch.from_numpy(waveform)\n",
        "  return torch.unsqueeze(waveform, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN_ampWvnf2o"
      },
      "source": [
        "## Tile waveform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GTMYtMsxNQ-l"
      },
      "outputs": [],
      "source": [
        "def tile(waveform, expected_time):\n",
        "  waveform_time = waveform.shape[1]\n",
        "  expected_time = expected_time\n",
        "  repeat_times = (expected_time // waveform_time) + 1\n",
        "  tiled_data = waveform.repeat(1, repeat_times)\n",
        "  return tiled_data[:, :expected_time]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Llwj01ppOe8"
      },
      "source": [
        "## Normalize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypArLytopQUH"
      },
      "outputs": [],
      "source": [
        "def standardize(waveform):\n",
        "  means = waveform.mean()\n",
        "  stds = waveform.std()\n",
        "  waveform = (waveform - means) / (stds + 1e-7)\n",
        "  return waveform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALWLzatbKI5p"
      },
      "source": [
        "# 📡 Data Loading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwpxxOKX0_AX"
      },
      "source": [
        "## Data loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7xC5XEGkKMD"
      },
      "outputs": [],
      "source": [
        "class get_data(Dataset):\n",
        "  def __init__(self, config,  paths, train):\n",
        "    self.speaker_out = [f\"{int(x):02}\" for x in config.fold]\n",
        "    self.train = train\n",
        "\n",
        "    self.paths =  paths\n",
        "    self.files = self.filter_files(self.paths[0])\n",
        "\n",
        "    self.length = config.max_length\n",
        "    self.sample_rate = config.sample_rate\n",
        "\n",
        "    self.labels_list = [self.get_label(file_name)[1] for file_name in self.files]\n",
        "    print(self.labels_list[:10])\n",
        "    self.num_classes = config.classes\n",
        "    self.majority_count = max([sum([item==label for item in self.labels_list]) for label in set(self.labels_list)])\n",
        "    self.prior_prob = [self.labels_list.count(label)/len(self.labels_list) for label in range(self.num_classes)]\n",
        "\n",
        "    self.aug_prob = config.aug_prob\n",
        "\n",
        "  def filter_files(self, path):\n",
        "    list_of_files = os.listdir(path)\n",
        "    if not self.train:\n",
        "      file_list = [item[:-3] for item in list_of_files if item.startswith(self.speaker_out[0])]\n",
        "      return file_list\n",
        "    elif self.train:\n",
        "      file_list = [item[:-3] for item in list_of_files if not(item.startswith(self.speaker_out[0]))]\n",
        "      return file_list\n",
        "\n",
        "  def get_wav(self, file):\n",
        "    waveform = get_waveform(file, self.sample_rate)\n",
        "    if waveform.shape[0] > 1:\n",
        "      waveform = torch.mean(waveform, dim=0).unsqueeze(0)\n",
        "    if self.train:\n",
        "      waveform = augment(waveform, self.aug_prob, self.sample_rate)\n",
        "    tiled_wav = tile(waveform, self.length)\n",
        "    std_wav = standardize(tiled_wav)\n",
        "    return std_wav\n",
        "\n",
        "  def get_features(self, path, durations):\n",
        "    short_x = torch.load(path)\n",
        "    x = []\n",
        "    for f_i, d_i in zip(short_x, durations):\n",
        "      x += [float(f_i)] * int(d_i)\n",
        "    x = torch.FloatTensor(x)\n",
        "    x = torch.unsqueeze(x, 0)\n",
        "    x = standardize(x)\n",
        "    x = tile(x, self.length)\n",
        "    return x\n",
        "\n",
        "  def get_label(self, file_name):\n",
        "    emo_dict = {\n",
        "    'W':0, # anger\n",
        "    'A':1, # anxiety/fear\n",
        "    'L':2, # boredom\n",
        "    'E':3, # disgust\n",
        "    'F':4, # happiness\n",
        "    'T':5, # sadness\n",
        "    'N':6, # neutral\n",
        "    }\n",
        "\n",
        "    gender_dict = {\n",
        "      '03': 0,\n",
        "      '08': 1,\n",
        "      '09': 1,\n",
        "      '10': 0,\n",
        "      '11': 0,\n",
        "      '12': 0,\n",
        "      '13': 1,\n",
        "      '14': 1,\n",
        "      '15': 0,\n",
        "      '16': 1\n",
        "    }\n",
        "    # emotion_char = file_name[6:8] #RAVDESS\n",
        "    emotion_char = file_name[5] # EMODB\n",
        "    # emotion_char = file_name[19] #IEMOCAP\n",
        "    # session_char = file_name[4]\n",
        "    # gender_char = file_name[15]\n",
        "    fold_char = file_name[:2]\n",
        "    label = int(int(fold_char) * 100 + int(gender_dict[fold_char]) * 10 + emo_dict[emotion_char])\n",
        "    return (emo_dict[emotion_char], label)\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.files)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    file_name = self.files[index]\n",
        "\n",
        "    features = []\n",
        "\n",
        "    for path in self.paths:\n",
        "      file_path = os.path.join(path, file_name + '.pt')\n",
        "      e = torch.load(file_path)\n",
        "      e = torch.squeeze(e)\n",
        "      e = rearrange(e, \"l c -> c l\")\n",
        "      e = tile(e, self.length)\n",
        "      e = rearrange(e, \"c l -> l c\")\n",
        "      features.append(e)\n",
        "\n",
        "    features = torch.stack(features, dim=0)\n",
        "\n",
        "    label = self.get_label(file_name)\n",
        "    return features, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lW7sm5jT1Bxd"
      },
      "outputs": [],
      "source": [
        "def make_loader(dataset, batch_size):\n",
        "  if dataset.train:\n",
        "    sampler = samplers.MPerClassSampler(labels=dataset.labels_list, m=batch_size//dataset.num_classes, batch_size=batch_size, length_before_new_iter=dataset.majority_count*dataset.num_classes)\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size,\n",
        "                                         sampler = sampler)\n",
        "  else:\n",
        "    loader = torch.utils.data.DataLoader(dataset=dataset,\n",
        "                                         batch_size=batch_size)\n",
        "  return loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbqFdFCnxWXU"
      },
      "source": [
        "# 🏗️ Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSBtNH4m62Si"
      },
      "source": [
        "## Attentive Stats Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57J7dbDR65zt"
      },
      "outputs": [],
      "source": [
        "class ASP(nn.Module):\n",
        "\n",
        "    def __init__(self, num_emdb, attn_dim=None):\n",
        "        super().__init__()\n",
        "        if not attn_dim:\n",
        "          attn_dim = num_emdb\n",
        "        self.asp = AttentiveStatisticsPooling(channels=num_emdb, attention_channels=attn_dim, global_context=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "\n",
        "        x = rearrange(x, \"b l c -> b c l\")\n",
        "        x = self.asp(x)\n",
        "        x = x.squeeze()\n",
        "\n",
        "        x, _ = torch.split(x, 768, dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYa5fZ3exKUN"
      },
      "source": [
        "## Maxout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sO6CKd4yxLmL"
      },
      "outputs": [],
      "source": [
        "class Maxout(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, pool_size):\n",
        "        super().__init__()\n",
        "        self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\n",
        "        self.lin = nn.Linear(d_in, d_out * pool_size)\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        shape = list(inputs.size())\n",
        "        shape[-1] = self.d_out\n",
        "        shape.append(self.pool_size)\n",
        "        max_dim = len(shape) - 1\n",
        "        out = self.lin(inputs)\n",
        "        m, i = out.view(*shape).max(max_dim)\n",
        "        return m"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8FCrLf0jcAo"
      },
      "source": [
        "## Classification Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RzDxpn2jekl"
      },
      "outputs": [],
      "source": [
        "class ClassificationHead(nn.Module):\n",
        "\n",
        "    def __init__(self, num_emdb, num_class, p=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.norm = nn.LayerNorm(normalized_shape = num_emdb)\n",
        "        self.dropout = nn.Dropout(p=p)\n",
        "        self.head = nn.Linear(num_emdb, num_class)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.head(self.dropout(self.norm(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgipQhHLqKWM"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jcq_qspkgHim"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.experts = nn.ModuleList([\n",
        "                                    nn.Sequential(\n",
        "                                        ASP(num_emdb = config.embeddings, attn_dim=config.time_attn_dim),\n",
        "                                        nn.Dropout(p=0.1),\n",
        "                                        Maxout(d_in = config.embeddings, d_out = config.hidden_size, pool_size = 3),\n",
        "                                        ClassificationHead(num_emdb= config.hidden_size, num_class= config.classes, p= config.proj_drop)\n",
        "                                    )\n",
        "                                    for _ in range(len(config.layers))\n",
        "                                    ]\n",
        "                                  )\n",
        "\n",
        "    def forward(self, x: torch.Tensor, extra_logs=False):\n",
        "\n",
        "        all_results = []\n",
        "        for i, expert in enumerate(self.experts):\n",
        "          all_results.append(expert(x[:,i]))\n",
        "        x = torch.stack(all_results, dim=1)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUvR194wKrQq"
      },
      "source": [
        "# 👟 Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "935OAzMWoaB6"
      },
      "outputs": [],
      "source": [
        "def train(model, train_loader, dev_loader, criterion, optimizer, scheduler, metric, config):\n",
        "    # Tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    wandb.watch(model, criterion, log=\"all\", log_freq=1)\n",
        "\n",
        "    best_acc   = 0.0\n",
        "\n",
        "    for epoch in tqdm(range(config.epochs)):\n",
        "\n",
        "      train_loss = []\n",
        "      train_acc_majority_vote  = []\n",
        "      train_acc_fixed_weighted_mean = []\n",
        "      train_acc_max_max = []\n",
        "      train_acc_distribution_summation = []\n",
        "      train_acc_dempster_shafer = []\n",
        "      train_acc_naive_bayes = []\n",
        "      train_acc_entropy_weighting = []\n",
        "      train_acc_density_based_weighting = []\n",
        "      train_acc_entropy_weighted_mean = []\n",
        "      dev_loss   = []\n",
        "      dev_acc_majority_vote  = []\n",
        "      dev_acc_fixed_weighted_mean = []\n",
        "      dev_acc_max_max = []\n",
        "      dev_acc_distribution_summation = []\n",
        "      dev_acc_dempster_shafer = []\n",
        "      dev_acc_naive_bayes = []\n",
        "      dev_acc_entropy_weighting = []\n",
        "      dev_acc_density_based_weighting = []\n",
        "      dev_acc_entropy_weighted_mean = []\n",
        "\n",
        "      for inputs, labels in train_loader:\n",
        "          loss, accuracies = train_batch(inputs, labels, model, optimizer, criterion, metric, config)\n",
        "          train_loss.append(loss.item())\n",
        "          train_acc_majority_vote.append(accuracies['majority_vote'].item())\n",
        "          train_acc_fixed_weighted_mean.append(accuracies['fixed_weighted_mean'].item())\n",
        "          train_acc_max_max.append(accuracies['max_max'].item())\n",
        "          train_acc_distribution_summation.append(accuracies['distribution_summation'].item())\n",
        "          train_acc_dempster_shafer.append(accuracies['dempster_shafer'].item())\n",
        "          train_acc_naive_bayes.append(accuracies['naive_bayes'].item())\n",
        "          train_acc_entropy_weighting.append(accuracies['entropy_weighting'].item())\n",
        "          train_acc_density_based_weighting.append(accuracies['density_based_weighting'].item())\n",
        "          train_acc_entropy_weighted_mean.append(accuracies['entropy_weighted_mean'].item())\n",
        "\n",
        "      for inputs, labels in dev_loader:\n",
        "\n",
        "          loss, accuracies = dev_batch(inputs, labels, model, criterion, metric, config)\n",
        "          dev_loss.append(loss.item())\n",
        "          dev_acc_majority_vote.append(accuracies['majority_vote'].item())\n",
        "          dev_acc_fixed_weighted_mean.append(accuracies['fixed_weighted_mean'].item())\n",
        "          dev_acc_max_max.append(accuracies['max_max'].item())\n",
        "          dev_acc_distribution_summation.append(accuracies['distribution_summation'].item())\n",
        "          dev_acc_dempster_shafer.append(accuracies['dempster_shafer'].item())\n",
        "          dev_acc_naive_bayes.append(accuracies['naive_bayes'].item())\n",
        "          dev_acc_entropy_weighting.append(accuracies['entropy_weighting'].item())\n",
        "          dev_acc_density_based_weighting.append(accuracies['density_based_weighting'].item())\n",
        "          dev_acc_entropy_weighted_mean.append(accuracies['entropy_weighted_mean'].item())\n",
        "\n",
        "      scheduler.step()\n",
        "\n",
        "      # Report metrics every epoch\n",
        "      wandb.log({\n",
        "            \"Train loss\": np.mean(train_loss),\n",
        "            \"Train Accuracy (Majority Vote)\": np.mean(train_acc_majority_vote),\n",
        "            \"Train Accuracy (Fixed-weighted Mean)\": np.mean(train_acc_fixed_weighted_mean),\n",
        "            \"Train Accuracy (Max of Max)\": np.mean(train_acc_max_max),\n",
        "            \"Train Accuracy (Distribution Summation)\": np.mean(train_acc_distribution_summation),\n",
        "            \"Train Accuracy (Dempster Shafer)\": np.mean(train_acc_dempster_shafer),\n",
        "            \"Train Accuracy (Naive Bayes)\": np.mean(train_acc_naive_bayes),\n",
        "            \"Train Accuracy (Entropy Weighting)\": np.mean(train_acc_entropy_weighting),\n",
        "            \"Train Accuracy (Density based Weighting)\": np.mean(train_acc_density_based_weighting),\n",
        "            \"Train Accuracy (Entropy-weighted Mean)\": np.mean(train_acc_entropy_weighted_mean),\n",
        "            \"Validation loss\": np.mean(dev_loss),\n",
        "            \"Validation Accuracy (Majority Vote)\": np.mean(dev_acc_majority_vote),\n",
        "            \"Validation Accuracy (Fixed-weighted Mean)\": np.mean(dev_acc_fixed_weighted_mean),\n",
        "            \"Validation Accuracy (Max of Max)\": np.mean(dev_acc_max_max),\n",
        "            \"Validation Accuracy (Distribution Summation)\": np.mean(dev_acc_distribution_summation),\n",
        "            \"Validation Accuracy (Dempster Shafer)\": np.mean(dev_acc_dempster_shafer),\n",
        "            \"Validation Accuracy (Naive Bayes)\": np.mean(dev_acc_naive_bayes),\n",
        "            \"Validation Accuracy (Entropy Weighting)\": np.mean(dev_acc_entropy_weighting),\n",
        "            \"Validation Accuracy (Density based Weighting)\": np.mean(dev_acc_density_based_weighting),\n",
        "            \"Validation Accuracy (Entropy-weighted Mean)\": np.mean(dev_acc_entropy_weighted_mean)\n",
        "            },\n",
        "          step=epoch)\n",
        "\n",
        "      # Keep best model\n",
        "      new_acc = np.mean([\n",
        "          np.mean(dev_acc_majority_vote),\n",
        "          np.mean(dev_acc_fixed_weighted_mean),\n",
        "          np.mean(dev_acc_max_max),\n",
        "          np.mean(dev_acc_distribution_summation),\n",
        "          np.mean(dev_acc_dempster_shafer),\n",
        "          np.mean(dev_acc_naive_bayes),\n",
        "          np.mean(dev_acc_entropy_weighting),\n",
        "          np.mean(dev_acc_density_based_weighting),\n",
        "          np.mean(dev_acc_entropy_weighted_mean)\n",
        "      ])\n",
        "      if new_acc > best_acc:\n",
        "        best_acc = max(best_acc, new_acc)\n",
        "        torch.save(model.state_dict(), 'model.pkl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGaH8QC7qtlL"
      },
      "outputs": [],
      "source": [
        "def train_batch(inputs, labels, model, optimizer, criterion, metric, config):\n",
        "\n",
        "    model = model.train()\n",
        "\n",
        "    # ensuring that the model is in train mode.\n",
        "    model.train()\n",
        "\n",
        "    # Forward pass ➡\n",
        "    outputs = model(inputs)\n",
        "    targets = process_labels(labels, criterion)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Backward pass ⬅\n",
        "    optimizer.zero_grad()\n",
        "    accelerator.backward(loss)\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracies = {}\n",
        "    accuracies['majority_vote'] = metric.calculate_acc(metric.majority_vote(outputs), labels)\n",
        "    accuracies['fixed_weighted_mean'] = metric.calculate_acc(metric.fixed_weighted_mean(outputs), labels)\n",
        "    accuracies['max_max'] = metric.calculate_acc(metric.max_max(outputs), labels)\n",
        "    accuracies['distribution_summation'] = metric.calculate_acc(metric.distribution_summation(outputs), labels)\n",
        "    accuracies['dempster_shafer'] = metric.calculate_acc(metric.dempster_shafer(outputs), labels)\n",
        "    accuracies['naive_bayes'] = metric.calculate_acc(metric.naive_bayes(outputs), labels)\n",
        "    accuracies['entropy_weighting'] = metric.calculate_acc(metric.entropy_weighting(outputs), labels)\n",
        "    accuracies['density_based_weighting'] = metric.calculate_acc(metric.density_based_weighting(outputs), labels)\n",
        "    accuracies['entropy_weighted_mean'] = metric.calculate_acc(metric.entropy_weighted_mean(outputs), labels)\n",
        "\n",
        "    return loss, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ft6vbEBdz1yU"
      },
      "outputs": [],
      "source": [
        "def dev_batch(inputs, labels, model, criterion, metric, config):\n",
        "    model = model.train()\n",
        "\n",
        "    # ensuring that the model is in inference mode.\n",
        "    model.eval()\n",
        "\n",
        "    # Forward pass ➡\n",
        "    outputs = model(inputs)\n",
        "    targets = process_labels(labels, criterion)\n",
        "    loss = criterion(outputs, targets)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracies = {}\n",
        "    accuracies['majority_vote'] = metric.calculate_acc(metric.majority_vote(outputs), labels)\n",
        "    accuracies['fixed_weighted_mean'] = metric.calculate_acc(metric.fixed_weighted_mean(outputs), labels)\n",
        "    accuracies['max_max'] = metric.calculate_acc(metric.max_max(outputs), labels)\n",
        "    accuracies['distribution_summation'] = metric.calculate_acc(metric.distribution_summation(outputs), labels)\n",
        "    accuracies['dempster_shafer'] = metric.calculate_acc(metric.dempster_shafer(outputs), labels)\n",
        "    accuracies['naive_bayes'] = metric.calculate_acc(metric.naive_bayes(outputs), labels)\n",
        "    accuracies['entropy_weighting'] = metric.calculate_acc(metric.entropy_weighting(outputs), labels)\n",
        "    accuracies['density_based_weighting'] = metric.calculate_acc(metric.density_based_weighting(outputs), labels)\n",
        "    accuracies['entropy_weighted_mean'] = metric.calculate_acc(metric.entropy_weighted_mean(outputs), labels)\n",
        "\n",
        "    return loss, accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo3y-aK56Lpo"
      },
      "outputs": [],
      "source": [
        "def process_labels(labels, criterion):\n",
        "  targets = labels\n",
        "  if str(criterion) in ['MultiLabelSoftMarginLoss()', 'FocalLoss()', 'BCEWithLogitsLoss()', 'AdditiveAngularMargin()']:\n",
        "    enc = OneHotEncoder()\n",
        "    enc.fit(np.array([0,1,2,3,4,5,6]).reshape(-1, 1))\n",
        "    cpu_labels = labels.cpu()\n",
        "    targets = torch.tensor(enc.transform(cpu_labels.reshape(-1, 1)).todense(), dtype=torch.float, device = device)\n",
        "  elif str(criterion) in ['LogSoftmaxWrapper(\\n  (loss_fn): AdditiveAngularMargin()\\n  (criterion): KLDivLoss()\\n)']:\n",
        "    targets = targets.unsqueeze(1)\n",
        "  return targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAA7DW7u8mnQ"
      },
      "source": [
        "# 🧪 Test and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9ewMS5l0xAS"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader, criterion, metric, config):\n",
        "  y_pred = []\n",
        "  pred_majority_vote  = []\n",
        "  pred_fixed_weighted_mean = []\n",
        "  pred_max_max = []\n",
        "  pred_distribution_summation = []\n",
        "  pred_dempster_shafer = []\n",
        "  pred_naive_bayes = []\n",
        "  pred_entropy_weighting = []\n",
        "  pred_density_based_weighting = []\n",
        "  pred_entropy_weighted_mean = []\n",
        "  y_true = []\n",
        "  expert_predictions = []\n",
        "\n",
        "  # ensuring that the model is in inference mode.\n",
        "  model = model.train()\n",
        "  model.eval()\n",
        "\n",
        "  # Run the model on some test examples\n",
        "  for inputs, labels in test_loader:\n",
        "      outputs = model(inputs, extra_logs=True)\n",
        "\n",
        "      y_true += labels[0].tolist()\n",
        "\n",
        "      pred_majority_vote += metric.majority_vote(outputs)\n",
        "      pred_fixed_weighted_mean += metric.fixed_weighted_mean(outputs)\n",
        "      pred_max_max += metric.max_max(outputs)\n",
        "      pred_distribution_summation += metric.distribution_summation(outputs)\n",
        "      pred_dempster_shafer += metric.dempster_shafer(outputs)\n",
        "      pred_naive_bayes += metric.naive_bayes(outputs)\n",
        "      pred_entropy_weighting += metric.entropy_weighting(outputs)\n",
        "      pred_density_based_weighting += metric.density_based_weighting(outputs)\n",
        "      pred_entropy_weighted_mean += metric.entropy_weighted_mean(outputs)\n",
        "\n",
        "      expert_predictions += outputs.argmax(dim=2).tolist()\n",
        "\n",
        "  # weighted and unweighted accuracy\n",
        "\n",
        "  WA = Accuracy(task = 'multiclass',\n",
        "                num_classes = config.classes,\n",
        "                average = 'micro')\n",
        "  UA = Accuracy(task = 'multiclass',\n",
        "                num_classes = config.classes,\n",
        "                average = 'macro')\n",
        "\n",
        "  # selected layers table\n",
        "  methods = ['Majority Vote' ,'Fixed-weighted Mean' ,'Max of Max' ,'Distribution Summation' ,'Dempster Shafer' ,'Naive Bayes' ,'Entropy Weighting' ,'Density based Weighting' ,'Entropy-weighted Mean']\n",
        "  y_pred += [list(x) for x in zip(\n",
        "      pred_majority_vote,\n",
        "      pred_fixed_weighted_mean,\n",
        "      pred_max_max,\n",
        "      pred_distribution_summation,\n",
        "      pred_dempster_shafer,\n",
        "      pred_naive_bayes,\n",
        "      pred_entropy_weighting,\n",
        "      pred_density_based_weighting,\n",
        "      pred_entropy_weighted_mean,\n",
        "      y_true\n",
        "  )]\n",
        "  my_data = [a + b for a, b in list(zip(expert_predictions, y_pred))]\n",
        "  columns=[f'Expert #{i+1} predictions' for i in range(len(config.layers))]+methods+['Target']\n",
        "\n",
        "\n",
        "  wandb.log({\n",
        "      \"Weighted Accuracy (Majority Vote)\": WA(torch.tensor(pred_majority_vote), torch.tensor(y_true)),\n",
        "      \"Unweighted Accuracy (Majority Vote)\": UA(torch.tensor(pred_majority_vote), torch.tensor(y_true)),\n",
        "      \"Weighted Accuracy (Fixed-weighted Mean)\": WA(torch.tensor(pred_fixed_weighted_mean), torch.tensor(y_true)),\n",
        "      \"Unweighted Accuracy (Fixed-weighted Mean)\": UA(torch.tensor(pred_fixed_weighted_mean), torch.tensor(y_true)),\n",
        "      \"Weighted Accuracy (Max of Max)\": WA(torch.tensor(pred_max_max), torch.tensor(y_true)),\n",
        "      \"Unweighted Accuracy (Max of Max)\": UA(torch.tensor(pred_max_max), torch.tensor(y_true)),\n",
        "      \"Weighted Accuracy (Distribution Summation)\": WA(torch.tensor(pred_distribution_summation), torch.tensor(y_true)),\n",
        "      \"Unweighted Accuracy (Distribution Summation)\": UA(torch.tensor(pred_distribution_summation), torch.tensor(y_true)),\n",
        "      \"Weighted Accuracy (Dempster Shafer)\": WA(torch.tensor(pred_dempster_shafer), torch.tensor(y_true)),\n",
        "      \"Unweighted Accuracy (Dempster Shafer)\": UA(torch.tensor(pred_dempster_shafer), torch.tensor(y_true)),\n",
        "      \"Weighted Accuracy (Naive Bayes)\": WA(torch.tensor(pred_naive_bayes), torch.tensor(y_true)),\n",
        "      \"Unweighted Accuracy (Naive Bayes)\": UA(torch.tensor(pred_naive_bayes), torch.tensor(y_true)),\n",
        "      \"Weighted Accuracy (Entropy Weighting)\": WA(torch.tensor(pred_entropy_weighting), torch.tensor(y_true)),\n",
        "      \"Unweighted Accuracy (Entropy Weighting)\": UA(torch.tensor(pred_entropy_weighting), torch.tensor(y_true)),\n",
        "      \"Weighted Accuracy (Density based Weighting)\": WA(torch.tensor(pred_density_based_weighting), torch.tensor(y_true)),\n",
        "      \"Unweighted Accuracy (Density based Weighting)\": UA(torch.tensor(pred_density_based_weighting), torch.tensor(y_true)),\n",
        "      \"Weighted Accuracy (Entropy-weighted Mean)\": WA(torch.tensor(pred_entropy_weighted_mean), torch.tensor(y_true)),\n",
        "      \"Unweighted Accuracy (Entropy-weighted Mean)\": UA(torch.tensor(pred_entropy_weighted_mean), torch.tensor(y_true)),\n",
        "      \"Confusion Matrix (Majority Vote)\" : wandb.plot.confusion_matrix(probs=None, y_true=y_true, preds=pred_majority_vote, class_names=config.class_names),\n",
        "      \"Confusion Matrix (Fixed-weighted Mean)\" : wandb.plot.confusion_matrix(probs=None, y_true=y_true, preds=pred_fixed_weighted_mean, class_names=config.class_names),\n",
        "      \"Confusion Matrix (Max of Max)\" : wandb.plot.confusion_matrix(probs=None, y_true=y_true, preds=pred_max_max, class_names=config.class_names),\n",
        "      \"Confusion Matrix (Distribution Summation)\" : wandb.plot.confusion_matrix(probs=None, y_true=y_true, preds=pred_distribution_summation, class_names=config.class_names),\n",
        "      \"Confusion Matrix (Dempster Shafer)\" : wandb.plot.confusion_matrix(probs=None, y_true=y_true, preds=pred_dempster_shafer, class_names=config.class_names),\n",
        "      \"Confusion Matrix (Naive Bayes)\" : wandb.plot.confusion_matrix(probs=None, y_true=y_true, preds=pred_naive_bayes, class_names=config.class_names),\n",
        "      \"Confusion Matrix (Entropy Weighting)\" : wandb.plot.confusion_matrix(probs=None, y_true=y_true, preds=pred_entropy_weighting, class_names=config.class_names),\n",
        "      \"Confusion Matrix (Density based Weighting)\" : wandb.plot.confusion_matrix(probs=None, y_true=y_true, preds=pred_density_based_weighting, class_names=config.class_names),\n",
        "      \"Confusion Matrix (Entropy-weighted Mean)\" : wandb.plot.confusion_matrix(probs=None, y_true=y_true, preds=pred_entropy_weighted_mean, class_names=config.class_names),\n",
        "      \"Selected Layers\": wandb.Table(data=my_data, columns=columns)\n",
        "      })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRQLHHoqqzPm"
      },
      "outputs": [],
      "source": [
        "def readable_targets(labels):\n",
        "  emo_dict = {\n",
        "      0:'anger',\n",
        "      1:'anxiety/fear',\n",
        "      2:'boredom',\n",
        "      3:'disgust',\n",
        "      4:'happiness',\n",
        "      5:'sadness',\n",
        "      6:'neutral',\n",
        "  }\n",
        "  str_labels = [emo_dict[x] for x in labels]\n",
        "\n",
        "  return str_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTBT0qub4gpt"
      },
      "source": [
        "# 🏃‍♀️ Run training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aft9a4W4jOf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "84cf05cf-29f6-4d29-ca91-f7db46c6a00a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: cl1fzebk with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattn_drop: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taug_prob: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclass_names: ['anger', 'anxiety/fear', 'boredom', 'disgust', 'happiness', 'neutral', 'sadness']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclasses: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: EMODB\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings: 768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \temo_emdb_ratio: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfold: ['03']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_channels: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_size: [12, 350, 768]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_attn_dim: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_selection: asp\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: AF\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 350\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_cycles: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_heads: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: AdamW\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tpath: kntu-asp-dl/EMODB/EMODB:wav2vec_fp16_noft_base\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tproj_drop: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject_name: Thesis\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsample_rate: 16000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_average: asp\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_attn_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.009\n",
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhalflingwizard\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230224_080723-cl1fzebk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/halflingwizard/Thesis/runs/cl1fzebk' target=\"_blank\">hardy-sweep-1</a></strong> to <a href='https://wandb.ai/halflingwizard/Thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/halflingwizard/Thesis/sweeps/e7czjb2r' target=\"_blank\">https://wandb.ai/halflingwizard/Thesis/sweeps/e7czjb2r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/halflingwizard/Thesis' target=\"_blank\">https://wandb.ai/halflingwizard/Thesis</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/halflingwizard/Thesis/sweeps/e7czjb2r' target=\"_blank\">https://wandb.ai/halflingwizard/Thesis/sweeps/e7czjb2r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/halflingwizard/Thesis/runs/cl1fzebk' target=\"_blank\">https://wandb.ai/halflingwizard/Thesis/runs/cl1fzebk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t1, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:23.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t2, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:21.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t3, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:21.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t4, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:20.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t5, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:20.8\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t6, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:20.9\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t7, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:22.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t8, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:21.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t9, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:20.6\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t10, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:20.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t11, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:20.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t12, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:20.6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[913, 913, 1004, 1105, 815, 810, 1206, 1504, 1415, 1501]\n",
            "[300, 305, 306, 300, 300, 300, 302, 304, 306, 306]\n",
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "Model                                                   [32, 12, 7]               --\n",
            "├─ModuleList: 1-1                                       --                        --\n",
            "│    └─Sequential: 2-1                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-1                                    [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-2                                [32, 768]                 --\n",
            "│    │    └─Maxout: 3-3                                 [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-4                     [32, 7]                   583\n",
            "│    └─Sequential: 2-2                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-5                                    [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-6                                [32, 768]                 --\n",
            "│    │    └─Maxout: 3-7                                 [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-8                     [32, 7]                   583\n",
            "│    └─Sequential: 2-3                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-9                                    [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-10                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-11                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-12                    [32, 7]                   583\n",
            "│    └─Sequential: 2-4                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-13                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-14                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-15                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-16                    [32, 7]                   583\n",
            "│    └─Sequential: 2-5                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-17                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-18                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-19                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-20                    [32, 7]                   583\n",
            "│    └─Sequential: 2-6                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-21                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-22                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-23                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-24                    [32, 7]                   583\n",
            "│    └─Sequential: 2-7                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-25                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-26                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-27                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-28                    [32, 7]                   583\n",
            "│    └─Sequential: 2-8                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-29                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-30                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-31                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-32                    [32, 7]                   583\n",
            "│    └─Sequential: 2-9                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-33                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-34                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-35                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-36                    [32, 7]                   583\n",
            "│    └─Sequential: 2-10                                 [32, 7]                   --\n",
            "│    │    └─ASP: 3-37                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-38                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-39                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-40                    [32, 7]                   583\n",
            "│    └─Sequential: 2-11                                 [32, 7]                   --\n",
            "│    │    └─ASP: 3-41                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-42                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-43                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-44                    [32, 7]                   583\n",
            "│    └─Sequential: 2-12                                 [32, 7]                   --\n",
            "│    │    └─ASP: 3-45                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-46                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-47                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-48                    [32, 7]                   583\n",
            "=========================================================================================================\n",
            "Total params: 11,234,388\n",
            "Trainable params: 11,234,388\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 105.89\n",
            "=========================================================================================================\n",
            "Input size (MB): 412.88\n",
            "Forward/backward pass size (MB): 688.63\n",
            "Params size (MB): 44.94\n",
            "Estimated Total Size (MB): 1146.44\n",
            "=========================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [04:17<00:00,  6.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Couldn't save the model.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy (Dempster Shafer)</td><td>▁▁▂▃▄▅▆▆▅▆▆▆▅▇▆▇▆▇▆▇▆▇▇▇▆█▇▆▇▆▇█▅▆█▆▆▆▇▇</td></tr><tr><td>Train Accuracy (Density based Weighting)</td><td>▁▁▂▃▄▅▆▆▅▆▆▆▅▇▆▇▆▇▆▇▆▇▇▇▆█▇▅▇▆▇█▅▆█▆▆▆▇▇</td></tr><tr><td>Train Accuracy (Distribution Summation)</td><td>▁▁▂▃▄▅▆▆▅▆▆▆▅▇▆▇▆▇▆▇▆▇▇▇▆█▇▆▇▆▇▇▅▆█▆▆▆▇▇</td></tr><tr><td>Train Accuracy (Entropy Weighting)</td><td>▁▂▂▃▄▅▆▆▅▆▆▆▄▇▆▇▆▇▆▇▆▇▇▆▆█▇▅▇▆▇▇▅▆█▆▆▆▆▇</td></tr><tr><td>Train Accuracy (Entropy-weighted Mean)</td><td>▁▁▂▃▄▅▆▆▅▆▆▆▄▇▆▇▆▇▆▇▆▇▇▇▆█▇▆▇▆▇█▅▆█▆▆▆▇▇</td></tr><tr><td>Train Accuracy (Fixed-weighted Mean)</td><td>▁▂▂▄▄▅▅▆▅▆▆▆▅▇▆▇▆▇▆▇▆▇▇▇▆█▇▆▇▆▇▇▅▆█▆▆▆▇▇</td></tr><tr><td>Train Accuracy (Majority Vote)</td><td>▁▁▂▃▄▅▆▆▅▆▆▆▅▇▆▇▆▇▆▇▆▇▇▇▆█▇▅▇▆▇█▅▆█▆▆▆▆▇</td></tr><tr><td>Train Accuracy (Max of Max)</td><td>▁▁▂▃▄▄▅▅▅▆▆▆▄▇▆▇▆▇▇▇▆▇▇▇▆█▇▆▇▆▇▇▅▆█▆▆▆▇█</td></tr><tr><td>Train Accuracy (Naive Bayes)</td><td>▆▆█▁▆███▃▆█▆▆█▆█▆███▆█▆█▆▆▃▆▃▃▆█▃▆█▃▆█▆█</td></tr><tr><td>Train loss</td><td>██▆▆▅▅▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Unweighted Accuracy (Dempster Shafer)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Density based Weighting)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Distribution Summation)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Entropy Weighting)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Entropy-weighted Mean)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Fixed-weighted Mean)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Majority Vote)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Max of Max)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Naive Bayes)</td><td>▁</td></tr><tr><td>Validation Accuracy (Dempster Shafer)</td><td>▁▁▂▃▅▅▄▆▅▅▅▄▆▆▇▇▇▆▇█▇▇▇██▇▇▇████████████</td></tr><tr><td>Validation Accuracy (Density based Weighting)</td><td>▁▂▃▄▆▅▅▆▆▆▅▅▆▇▇█▇▆▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>Validation Accuracy (Distribution Summation)</td><td>▁▁▂▃▆▅▄▆▅▅▅▄▆▆▇▇▇▆▇█▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>Validation Accuracy (Entropy Weighting)</td><td>▁▂▃▄▆▅▅▆▆▆▅▅▆▆▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇▇████▇▇████</td></tr><tr><td>Validation Accuracy (Entropy-weighted Mean)</td><td>▁▁▂▃▅▅▄▆▅▅▅▄▆▆▇▇▇▆▇█▇▇▇█▇▇▇▇████████████</td></tr><tr><td>Validation Accuracy (Fixed-weighted Mean)</td><td>▁▁▂▃▅▅▃▄▅▅▅▄▅▆▇▇▇▆▇█▇▇▇██▇▇▇▇███████████</td></tr><tr><td>Validation Accuracy (Majority Vote)</td><td>▁▂▃▄▅▅▅▆▆▆▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>Validation Accuracy (Max of Max)</td><td>▁▁▃▄▄▄▄▄▅▅▅▄▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>Validation Accuracy (Naive Bayes)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation loss</td><td>█▇▆▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Weighted Accuracy (Dempster Shafer)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Density based Weighting)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Distribution Summation)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Entropy Weighting)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Entropy-weighted Mean)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Fixed-weighted Mean)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Majority Vote)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Max of Max)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Naive Bayes)</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy (Dempster Shafer)</td><td>0.79167</td></tr><tr><td>Train Accuracy (Density based Weighting)</td><td>0.79167</td></tr><tr><td>Train Accuracy (Distribution Summation)</td><td>0.79167</td></tr><tr><td>Train Accuracy (Entropy Weighting)</td><td>0.78571</td></tr><tr><td>Train Accuracy (Entropy-weighted Mean)</td><td>0.78571</td></tr><tr><td>Train Accuracy (Fixed-weighted Mean)</td><td>0.79167</td></tr><tr><td>Train Accuracy (Majority Vote)</td><td>0.79762</td></tr><tr><td>Train Accuracy (Max of Max)</td><td>0.78571</td></tr><tr><td>Train Accuracy (Naive Bayes)</td><td>0.14286</td></tr><tr><td>Train loss</td><td>8.92001</td></tr><tr><td>Unweighted Accuracy (Dempster Shafer)</td><td>0.93061</td></tr><tr><td>Unweighted Accuracy (Density based Weighting)</td><td>0.93061</td></tr><tr><td>Unweighted Accuracy (Distribution Summation)</td><td>0.93061</td></tr><tr><td>Unweighted Accuracy (Entropy Weighting)</td><td>0.89796</td></tr><tr><td>Unweighted Accuracy (Entropy-weighted Mean)</td><td>0.93061</td></tr><tr><td>Unweighted Accuracy (Fixed-weighted Mean)</td><td>0.93061</td></tr><tr><td>Unweighted Accuracy (Majority Vote)</td><td>0.93878</td></tr><tr><td>Unweighted Accuracy (Max of Max)</td><td>0.93878</td></tr><tr><td>Unweighted Accuracy (Naive Bayes)</td><td>0.14286</td></tr><tr><td>Validation Accuracy (Dempster Shafer)</td><td>0.85476</td></tr><tr><td>Validation Accuracy (Density based Weighting)</td><td>0.85476</td></tr><tr><td>Validation Accuracy (Distribution Summation)</td><td>0.85476</td></tr><tr><td>Validation Accuracy (Entropy Weighting)</td><td>0.82619</td></tr><tr><td>Validation Accuracy (Entropy-weighted Mean)</td><td>0.85476</td></tr><tr><td>Validation Accuracy (Fixed-weighted Mean)</td><td>0.84048</td></tr><tr><td>Validation Accuracy (Majority Vote)</td><td>0.86429</td></tr><tr><td>Validation Accuracy (Max of Max)</td><td>0.86429</td></tr><tr><td>Validation Accuracy (Naive Bayes)</td><td>0.14286</td></tr><tr><td>Validation loss</td><td>10.89195</td></tr><tr><td>Weighted Accuracy (Dempster Shafer)</td><td>0.93878</td></tr><tr><td>Weighted Accuracy (Density based Weighting)</td><td>0.93878</td></tr><tr><td>Weighted Accuracy (Distribution Summation)</td><td>0.93878</td></tr><tr><td>Weighted Accuracy (Entropy Weighting)</td><td>0.85714</td></tr><tr><td>Weighted Accuracy (Entropy-weighted Mean)</td><td>0.93878</td></tr><tr><td>Weighted Accuracy (Fixed-weighted Mean)</td><td>0.93878</td></tr><tr><td>Weighted Accuracy (Majority Vote)</td><td>0.93878</td></tr><tr><td>Weighted Accuracy (Max of Max)</td><td>0.93878</td></tr><tr><td>Weighted Accuracy (Naive Bayes)</td><td>0.28571</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">hardy-sweep-1</strong> at: <a href='https://wandb.ai/halflingwizard/Thesis/runs/cl1fzebk' target=\"_blank\">https://wandb.ai/halflingwizard/Thesis/runs/cl1fzebk</a><br/>Synced 5 W&B file(s), 10 media file(s), 13 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230224_080723-cl1fzebk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: qtlvheba with config:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \talpha: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tattn_drop: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \taug_prob: 0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclass_names: ['anger', 'anxiety/fear', 'boredom', 'disgust', 'happiness', 'neutral', 'sadness']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tclasses: 7\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tdataset: EMODB\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tembeddings: 768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \temo_emdb_ratio: 0.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 40\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tfold: ['15']\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 64\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_channels: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tinput_size: [12, 350, 768]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_attn_dim: 32\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayer_selection: asp\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlayers: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.005\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: AF\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmargin: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tmax_length: 350\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_cycles: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_heads: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: AdamW\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tpath: kntu-asp-dl/EMODB/EMODB:wav2vec_fp16_noft_base\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tproj_drop: 0.2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tproject_name: Thesis\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tsample_rate: 16000\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tscale: 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttemporal_average: asp\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \ttime_attn_dim: 256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \twarmup_steps: 10\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.009\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230224_081729-qtlvheba</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/halflingwizard/Thesis/runs/qtlvheba' target=\"_blank\">solar-sweep-9</a></strong> to <a href='https://wandb.ai/halflingwizard/Thesis' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>Sweep page: <a href='https://wandb.ai/halflingwizard/Thesis/sweeps/e7czjb2r' target=\"_blank\">https://wandb.ai/halflingwizard/Thesis/sweeps/e7czjb2r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/halflingwizard/Thesis' target=\"_blank\">https://wandb.ai/halflingwizard/Thesis</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View sweep at <a href='https://wandb.ai/halflingwizard/Thesis/sweeps/e7czjb2r' target=\"_blank\">https://wandb.ai/halflingwizard/Thesis/sweeps/e7czjb2r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/halflingwizard/Thesis/runs/qtlvheba' target=\"_blank\">https://wandb.ai/halflingwizard/Thesis/runs/qtlvheba</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t1, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t2, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t3, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t4, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t5, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t6, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t7, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t8, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t9, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t10, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t11, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact EMODB:wav2vec_fp16_noft_base_t12, 108.71MB. 535 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   535 of 535 files downloaded.  \n",
            "Done. 0:0:0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[913, 913, 1004, 300, 1105, 815, 810, 1206, 1415, 1316]\n",
            "[1504, 1501, 1500, 1500, 1506, 1506, 1506, 1500, 1500, 1503]\n",
            "=========================================================================================================\n",
            "Layer (type:depth-idx)                                  Output Shape              Param #\n",
            "=========================================================================================================\n",
            "Model                                                   [32, 12, 7]               --\n",
            "├─ModuleList: 1-1                                       --                        --\n",
            "│    └─Sequential: 2-1                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-1                                    [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-2                                [32, 768]                 --\n",
            "│    │    └─Maxout: 3-3                                 [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-4                     [32, 7]                   583\n",
            "│    └─Sequential: 2-2                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-5                                    [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-6                                [32, 768]                 --\n",
            "│    │    └─Maxout: 3-7                                 [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-8                     [32, 7]                   583\n",
            "│    └─Sequential: 2-3                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-9                                    [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-10                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-11                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-12                    [32, 7]                   583\n",
            "│    └─Sequential: 2-4                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-13                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-14                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-15                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-16                    [32, 7]                   583\n",
            "│    └─Sequential: 2-5                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-17                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-18                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-19                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-20                    [32, 7]                   583\n",
            "│    └─Sequential: 2-6                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-21                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-22                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-23                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-24                    [32, 7]                   583\n",
            "│    └─Sequential: 2-7                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-25                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-26                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-27                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-28                    [32, 7]                   583\n",
            "│    └─Sequential: 2-8                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-29                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-30                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-31                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-32                    [32, 7]                   583\n",
            "│    └─Sequential: 2-9                                  [32, 7]                   --\n",
            "│    │    └─ASP: 3-33                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-34                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-35                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-36                    [32, 7]                   583\n",
            "│    └─Sequential: 2-10                                 [32, 7]                   --\n",
            "│    │    └─ASP: 3-37                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-38                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-39                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-40                    [32, 7]                   583\n",
            "│    └─Sequential: 2-11                                 [32, 7]                   --\n",
            "│    │    └─ASP: 3-41                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-42                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-43                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-44                    [32, 7]                   583\n",
            "│    └─Sequential: 2-12                                 [32, 7]                   --\n",
            "│    │    └─ASP: 3-45                                   [32, 768]                 787,968\n",
            "│    │    └─Dropout: 3-46                               [32, 768]                 --\n",
            "│    │    └─Maxout: 3-47                                [32, 64]                  147,648\n",
            "│    │    └─ClassificationHead: 3-48                    [32, 7]                   583\n",
            "=========================================================================================================\n",
            "Total params: 11,234,388\n",
            "Trainable params: 11,234,388\n",
            "Non-trainable params: 0\n",
            "Total mult-adds (G): 105.89\n",
            "=========================================================================================================\n",
            "Input size (MB): 412.88\n",
            "Forward/backward pass size (MB): 688.63\n",
            "Params size (MB): 44.94\n",
            "Estimated Total Size (MB): 1146.44\n",
            "=========================================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40/40 [04:17<00:00,  6.45s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Couldn't save the model.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy (Dempster Shafer)</td><td>▁▂▂▃▅▅▆▆▆▆▅▆▆▅▇▆▇▇▇▇▆█▇██▇▇█▇▇▇▆▆▆▇█▇▇▇█</td></tr><tr><td>Train Accuracy (Density based Weighting)</td><td>▁▂▂▃▅▅▆▆▆▇▆▆▆▅█▆▇▇▇▇▆█▇██▇▇██▇▇▆▆▆▇█▇▇▇█</td></tr><tr><td>Train Accuracy (Distribution Summation)</td><td>▁▂▂▃▅▅▆▆▆▆▅▆▆▅▇▆▇▇▇▇▆█▇██▇▇█▇▇▇▆▆▆▇█▇▇▇█</td></tr><tr><td>Train Accuracy (Entropy Weighting)</td><td>▁▂▂▃▅▅▆▅▆▆▆▆▆▅█▆▇▇▇▇▆█▇██▇▇██▇▇▆▆▆▇█▇▇▇█</td></tr><tr><td>Train Accuracy (Entropy-weighted Mean)</td><td>▁▂▂▃▅▅▆▅▆▆▅▆▆▅▇▆▇▇▇▇▆█▇▇█▇▇█▇▇▇▆▆▆▇█▇▇▇▇</td></tr><tr><td>Train Accuracy (Fixed-weighted Mean)</td><td>▁▂▂▃▅▅▆▆▆▆▅▆▆▅▇▆▇▇▇▇▆█▇██▇▇█▇▇▇▆▆▆▇█▇▇▇█</td></tr><tr><td>Train Accuracy (Majority Vote)</td><td>▁▂▂▃▅▅▆▆▆▇▆▆▆▅█▆▇▇▇▇▆█▇██▇▇██▇▇▆▆▆▇█▇▇▇█</td></tr><tr><td>Train Accuracy (Max of Max)</td><td>▁▂▂▃▅▄▅▅▆▆▅▆▆▅▇▆▇▇▇▆▆█▆▇█▇▇█▇▇▇▆▆▆▇█▇▇▇█</td></tr><tr><td>Train Accuracy (Naive Bayes)</td><td>█▆▆▆▁███▁█▆▃▆▆█▆▆██▃█▆███▆███▆█▆█▃▆█▃▃█▆</td></tr><tr><td>Train loss</td><td>██▇▆▅▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Unweighted Accuracy (Dempster Shafer)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Density based Weighting)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Distribution Summation)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Entropy Weighting)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Entropy-weighted Mean)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Fixed-weighted Mean)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Majority Vote)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Max of Max)</td><td>▁</td></tr><tr><td>Unweighted Accuracy (Naive Bayes)</td><td>▁</td></tr><tr><td>Validation Accuracy (Dempster Shafer)</td><td>▁▃▃▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇████▇▇▇██████████████</td></tr><tr><td>Validation Accuracy (Density based Weighting)</td><td>▁▃▃▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇████▇▇▇██████████████</td></tr><tr><td>Validation Accuracy (Distribution Summation)</td><td>▁▂▃▄▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇████▇▇▇██████████████</td></tr><tr><td>Validation Accuracy (Entropy Weighting)</td><td>▁▃▄▄▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████▇▇▇▇██▇██████████</td></tr><tr><td>Validation Accuracy (Entropy-weighted Mean)</td><td>▁▃▃▄▅▅▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇██▇▇▇██▇█▇▇▇███████</td></tr><tr><td>Validation Accuracy (Fixed-weighted Mean)</td><td>▁▃▃▄▅▆▇▇█▇▇▇▇▇▇▇▇▇▇████▇▇▇██▇███████████</td></tr><tr><td>Validation Accuracy (Majority Vote)</td><td>▁▃▄▄▅▆▆▆▇▇▇▇▇▇▆▇▇▇▇████▇▇▇██████████████</td></tr><tr><td>Validation Accuracy (Max of Max)</td><td>▁▃▂▄▄▄▅▆▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇▇▇▇█▇███▇██████</td></tr><tr><td>Validation Accuracy (Naive Bayes)</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Validation loss</td><td>█▆▅▅▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Weighted Accuracy (Dempster Shafer)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Density based Weighting)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Distribution Summation)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Entropy Weighting)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Entropy-weighted Mean)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Fixed-weighted Mean)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Majority Vote)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Max of Max)</td><td>▁</td></tr><tr><td>Weighted Accuracy (Naive Bayes)</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Accuracy (Dempster Shafer)</td><td>0.80952</td></tr><tr><td>Train Accuracy (Density based Weighting)</td><td>0.79762</td></tr><tr><td>Train Accuracy (Distribution Summation)</td><td>0.80952</td></tr><tr><td>Train Accuracy (Entropy Weighting)</td><td>0.80952</td></tr><tr><td>Train Accuracy (Entropy-weighted Mean)</td><td>0.79762</td></tr><tr><td>Train Accuracy (Fixed-weighted Mean)</td><td>0.80952</td></tr><tr><td>Train Accuracy (Majority Vote)</td><td>0.79762</td></tr><tr><td>Train Accuracy (Max of Max)</td><td>0.79762</td></tr><tr><td>Train Accuracy (Naive Bayes)</td><td>0.09524</td></tr><tr><td>Train loss</td><td>9.24371</td></tr><tr><td>Unweighted Accuracy (Dempster Shafer)</td><td>0.95833</td></tr><tr><td>Unweighted Accuracy (Density based Weighting)</td><td>0.95833</td></tr><tr><td>Unweighted Accuracy (Distribution Summation)</td><td>0.95833</td></tr><tr><td>Unweighted Accuracy (Entropy Weighting)</td><td>0.95833</td></tr><tr><td>Unweighted Accuracy (Entropy-weighted Mean)</td><td>0.98214</td></tr><tr><td>Unweighted Accuracy (Fixed-weighted Mean)</td><td>0.95833</td></tr><tr><td>Unweighted Accuracy (Majority Vote)</td><td>0.95833</td></tr><tr><td>Unweighted Accuracy (Max of Max)</td><td>0.98214</td></tr><tr><td>Unweighted Accuracy (Naive Bayes)</td><td>0.14286</td></tr><tr><td>Validation Accuracy (Dempster Shafer)</td><td>0.89643</td></tr><tr><td>Validation Accuracy (Density based Weighting)</td><td>0.89643</td></tr><tr><td>Validation Accuracy (Distribution Summation)</td><td>0.89643</td></tr><tr><td>Validation Accuracy (Entropy Weighting)</td><td>0.89643</td></tr><tr><td>Validation Accuracy (Entropy-weighted Mean)</td><td>0.91429</td></tr><tr><td>Validation Accuracy (Fixed-weighted Mean)</td><td>0.89643</td></tr><tr><td>Validation Accuracy (Majority Vote)</td><td>0.89643</td></tr><tr><td>Validation Accuracy (Max of Max)</td><td>0.91429</td></tr><tr><td>Validation Accuracy (Naive Bayes)</td><td>0.14286</td></tr><tr><td>Validation loss</td><td>10.87145</td></tr><tr><td>Weighted Accuracy (Dempster Shafer)</td><td>0.96429</td></tr><tr><td>Weighted Accuracy (Density based Weighting)</td><td>0.96429</td></tr><tr><td>Weighted Accuracy (Distribution Summation)</td><td>0.96429</td></tr><tr><td>Weighted Accuracy (Entropy Weighting)</td><td>0.96429</td></tr><tr><td>Weighted Accuracy (Entropy-weighted Mean)</td><td>0.98214</td></tr><tr><td>Weighted Accuracy (Fixed-weighted Mean)</td><td>0.96429</td></tr><tr><td>Weighted Accuracy (Majority Vote)</td><td>0.96429</td></tr><tr><td>Weighted Accuracy (Max of Max)</td><td>0.98214</td></tr><tr><td>Weighted Accuracy (Naive Bayes)</td><td>0.23214</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">solar-sweep-9</strong> at: <a href='https://wandb.ai/halflingwizard/Thesis/runs/qtlvheba' target=\"_blank\">https://wandb.ai/halflingwizard/Thesis/runs/qtlvheba</a><br/>Synced 5 W&B file(s), 10 media file(s), 13 artifact file(s) and 2 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20230224_081729-qtlvheba/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Exiting.\n"
          ]
        }
      ],
      "source": [
        "# Build, train and analyze the model with the pipeline\n",
        "wandb.agent(sweep_id,project=parameters_dict['project_name']['value'],entity='halflingwizard',function=model_pipeline)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "jvCxNvYU2qJ4",
        "CUURyymWaoia",
        "dkMn9HuKnSub",
        "xClzeKHPos32",
        "IN_ampWvnf2o",
        "2Llwj01ppOe8",
        "SSBtNH4m62Si",
        "iYa5fZ3exKUN",
        "D8FCrLf0jcAo"
      ],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
